---
title: "09. Unsupervised Learning II"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "../css/note-style.css"
---

```{r, message=FALSE, echo=FALSE}
source("../funs/funs.R")
options(dsst.traceit = FALSE)
```

## Load the Data

For today's notes we will work with one of the data sets from Project 2,
specifically the reviews of Music CDs.

```{r, message=FALSE}
docs <- read_csv("../data/amazon_cds.csv.bz2")
anno <- read_csv("../data/amazon_cds_token.csv.bz2")
```

Today, we will continue are study of unsupervised learning by studying a few
extensions of what we saw in the previous notes.

## Clusters with K-means

One thing that we often do when looking at plots of dimensionality reduction
is to look for clumps of documents that co-occur. We can do this explicit
by clustering the data using a clustering algorithm. Here, we will use a popular
option called K-means (note, this is not the same as k-NN, but I suppose it is
not entirely unrelated). It is an iterative algorithm that works as follows:

1. Pick the number of clusters N that you want to detect.
2. Randomly choose N data points as the starting centroids of each cluster.
3. Compute the distance of every data point to the centroids of the current
clusters.
4. Assign each data point to the cluster whose centroids it is closest.
5. Re-compute the cluster centroids as the average value of all the points in a
cluster.
6. Take the new cluster centroids, and repeat the process (computer distances,
reassign to groups, and recompute the centroids) iteratively until convergence.

The algorithm is not entirely deterministic because of the random starting
points. Typically, the algorithm is run several times and the "best" clustering
is chosen. How do we define the "best" in the case of a clustering algorithm?
A typical method is to measure the sum of squared distances to the cluster
centroids, a quantity that a good clustering will minimise.

Usually, K-means is run on a set of PCA coordinates rather than the entire
embedding itself. Often you will find that including a few dozen PCA components
provides a better fit (though it is rarely useful to use the entire TF-IDF).

To apply K-means, we will use the `dsst_clusters` function. We need to set the
number of clusters; it is also possible to change the number PCA dimensions.

```{r}
dsst_kmeans(anno, docs, doc_var = "label", n_dims = 2, n_clusters = 5L)
```

Notice that these closely compare to the clusters you would see by grouping
nearby points in the PCA plot from last time.

## Word Relationships

In the preceding analyses, we have focused on the analysis of the
document their usage of words. It turns out that it is also possible to apply
dimensionality reduction and distance metrics by swapping the words and the
documents. All of the concepts work the same, but now we have term frequencies
tell us how often each word is used in every document.

I included an easy-to-use argument to the `dsst_pca` function to compute the
PCA fit of this transposed analysis (as well as UMAP):

```{r, warning=FALSE}
dsst_pca(anno, invert = TRUE) %>% dsst_plot_dred()
```

As well, we can compute clusters of words:

```{r, warning=FALSE}
anno %>%
  filter(upos %in% c("NOUN")) %>%
  dsst_kmeans(invert = TRUE, n_clusters = 5L, n_dims = 15L)
```

Do you see any other interesting patterns here?
