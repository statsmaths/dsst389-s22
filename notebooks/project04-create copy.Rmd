---
title: "Project 04 --- Create Data"
output: html_document
---

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")
source("../funs/funs_custom.R")
```

```{r}
obj <- dsst_wiki_load("List_of_Walt_Disney_Studios_films_(1960–1979)")
links <- dsst_wiki_get_links_table(obj, table_num = 3L, column_num = 2L)
```

```{r}
get_disney_data <- function(obj, table_num)
{
  tree <- xml2::read_html(obj$parse$text[[1]])
  tables <- xml2::xml_find_all(tree, xpath = ".//table")
  
  this_table <- tables[[table_num]]
  rows <- xml2::xml_find_all(this_table, ".//tr")
  dates <- xml2::xml_text(xml2::xml_find_first(rows, ".//td"))
  
  links <- map(rows, function(u) {
        td <- xml2::xml_find_all(u, ".//td")
        res <- ifelse(length(td) < 2L, "",
                      xml2::xml_attr(xml2::xml_find_all(td[[2L]], ".//a"),
                                     "href"))
        res
      })
  
  links <- tibble(links = unlist(links), dates = dates)
  links
}

obj <- dsst_wiki_load("List_of_Walt_Disney_Studios_films_(1960–1979)")
get_disney_data(obj, table_num = 1L)
```





```{r}
docs <- unique(docs)

links <- links %>%
  filter(!stri_detect(links, fixed = ":")) 
```


```{r}
obj <- dsst_wiki_load("List_of_cities_by_international_visitors")

docs$train_id <- "train"
docs$label <- "Middle"
docs$label[1:30] <- "Most"
docs$label[75:138] <- "Least"
```


You should use this file to create your dataset, and then use the other file
to read in your data and run the analysis as with the other projects. This
will help avoid needing to create the data set each time you are working with
the project.

```{r}
obj <- dsst_wiki_load("List_of_fantasy_novels_(A–H)")
links1 <- dsst_wiki_get_links(obj, xpath = ".//li//a")
obj <- dsst_wiki_load("List_of_fantasy_novels_(I–R)")
links2 <- dsst_wiki_get_links(obj, xpath = ".//li//a")
obj <- dsst_wiki_load("List_of_fantasy_novels_(S–Z)")
links3 <- dsst_wiki_get_links(obj, xpath = ".//li//a")

links <- bind_rows(links1, links2, links3)
links
```



Here, select a starter page. I'll fill in my example from the notes:

```{r}
obj <- dsst_wiki_load("List_of_United_States_Olympic_medalists")
```

Now, look at some of the links. Note that you may need to change the code here
based on your starting page.

```{r}
dsst_wiki_get_links_table(obj, table_num = 3L, column_num = 2)
```


```{r, message=TRUE}
links <- dsst_wiki_get_links(obj, xpath = ".//table//a")
links
```

```{r, message=TRUE} 
obj <- dsst_wiki_load("List_of_United_States_Olympic_medalists")

links <- dsst_wiki_get_links(obj, xpath = ".//table//a")

links <- links %>%
  filter(!stri_detect(links, fixed = "Olympics")) %>%
  filter(!stri_detect(links, fixed = "national")) %>%
  filter(!stri_detect(links, fixed = "United")) %>%
  filter(!stri_detect(links, fixed = ":")) 

links
```

Next, grab all of the pages associated with the links you grabbed. This will
take a while the first time you run it.

```{r, message=TRUE}
docs <- dsst_wiki_make_data(links)
docs
```

```{r}
docs <- docs %>%
  filter(stri_detect(doc_id, fixed = "<i>"))
```

```{r}
docs <- docs %>%
  mutate(doc_id = stri_replace_all(doc_id, "", fixed = "<i>")) %>%
  mutate(doc_id = stri_replace_all(doc_id, "", fixed = "</i>")) 
```


We can then run the udpipe annotator over your data. It will be a bit slower
and not quite as accurate, as the spaCy annotator, but does not require setting
up Python on your machine (a real pain for just this one assignment).

```{r}
library(cleanNLP)
cnlp_init_udpipe("english")

docs <- filter(docs, stringi::stri_length(text) > 0)
anno <- cnlp_annotate(docs)$token
```

Here is a good sanity check, looking at the TF-IDF scores:

```{r}
dsst_tfidf(anno)
```

Finally, we can save our data set here:

```{r}
write_csv(docs, file.path("..", "data_project", "wiki.csv"))
write_csv(anno, file.path("..", "data_project", "wiki_token.csv.gz"))
```

You can then read back into R using the code in `project04.Rmd`. If you don't
find the results very interesting, or need to adjust anything, you can do it
here

```{r}
# writexl::write_xlsx(select(docs, -text), path = "../data_project/docs_manual_add.xlsx")
docs_new <- readxl::read_xlsx("../data_project/docs_manual_add.xlsx") %>%
  inner_join(docs, by = "doc_id")
```

```{r}
tokens %>%
  filter(upos == "NUM") %>%
  mutate(token = as.numeric(token)) %>%
  filter(!is.na(token)) %>%
  filter(token > 1800)

dsst_lda_build()
```

```{r}
docs <- read_csv(file.path("..", "data_project", "wiki.csv"))
anno <- read_csv(file.path("..", "data_project", "wiki_token.csv.gz"))
```

```{r}
to_remove <- anno %>%
  select(doc_id, lemma) %>%
  unique() %>%
  group_by(lemma) %>%
  summarise(prop = n() / nrow(docs)) %>%
  arrange(desc(prop)) %>%
  filter(prop > 0.95)

anno_new <- anno %>%
  anti_join(to_remove, by = "lemma")
```

```{r}
anno %>%
  group_by(doc_id) %>%
  summarize(n_terms = n()) %>%
  ggplot(aes(x = n_terms)) +
    geom_histogram(bins = 40, color = "black", fill = "white")
```

```{r}
docs %>%
  mutate(kill_count = "low") %>%
  mutate(kill_count = if_else(count > 40, "medium", kill_count)) %>%
  mutate(kill_count = if_else(count > 100, "high", kill_count))
```



